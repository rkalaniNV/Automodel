---
description: "Complete guide to NeMo Automodel repository and package structure, from high-level organization to detailed module hierarchy and development patterns."
tags: ["repository", "package", "structure", "development", "components", "architecture"]
categories: ["architecture", "development"]
---

(repository-and-package-guide)=
# Repository & Package Guide

Comprehensive guide to the NeMo Automodel codebase organization, from repository structure to package internals and development workflows.

## Guide Overview

This guide provides complete coverage of NeMo Automodel's organization:

::::{grid} 1 1 2 2
:gutter: 2

:::{grid-item-card} {octicon}`repo;1.5em;sd-mr-1` Repository Structure
:link: #repository-structure
:link-type: ref

Top-level repository organization, directories, and development workflow
+++
{bdg-primary}`Start Here`
:::

:::{grid-item-card} {octicon}`package;1.5em;sd-mr-1` Package Structure  
:link: #package-structure
:link-type: ref

Deep dive into `nemo_automodel/` module hierarchy and component details
+++
{bdg-info}`Technical Details`
:::

:::{grid-item-card} {octicon}`mortar-board;1.5em;sd-mr-1` Getting Started Paths
:link: #getting-started-paths
:link-type: ref

Recommended exploration paths for different user types
+++
{bdg-success}`Navigation`
:::

:::{grid-item-card} {octicon}`tools;1.5em;sd-mr-1` Development Patterns
:link: #development-patterns
:link-type: ref

Best practices, design principles, and contribution guidelines
+++
{bdg-warning}`Contributors`
:::

::::

## Repository Structure

The NeMo Automodel repository is organized into several top-level directories, each serving a specific purpose in the development, testing, and deployment workflow.

### Complete Repository Overview

```
Automodel/
â”œâ”€â”€ nemo_automodel/          # Core source code (main package)
â”‚   â”œâ”€â”€ components/          # Modular training components  
â”‚   â”œâ”€â”€ recipes/             # End-to-end training workflows
â”‚   â”œâ”€â”€ _cli/                # Command-line interface
â”‚   â””â”€â”€ shared/              # Cross-cutting utilities
â”œâ”€â”€ examples/                # Working YAML configs & sample scripts
â”‚   â”œâ”€â”€ llm/                 # Language model examples
â”‚   â””â”€â”€ vlm/                 # Vision-language model examples  
â”œâ”€â”€ tests/                   # Comprehensive test suites
â”‚   â”œâ”€â”€ unit_tests/          # Component-level tests
â”‚   â””â”€â”€ functional_tests/    # End-to-end integration tests
â”œâ”€â”€ docs/                    # Documentation source & extensions
â”‚   â”œâ”€â”€ _extensions/         # Custom Sphinx extensions
â”‚   â””â”€â”€ guides/              # User guides and tutorials
â”œâ”€â”€ docker/                  # Container definitions & scripts
â”œâ”€â”€ scripts/                 # Development utilities & tools
â”œâ”€â”€ LICENSE                  # Apache 2.0 license
â”œâ”€â”€ CONTRIBUTING.md          # Contribution guidelines
â”œâ”€â”€ pyproject.toml          # Package configuration
â””â”€â”€ README.md               # Project overview
```

### Directory Descriptions

```{list-table}
:header-rows: 1
:widths: 20 30 50

* - Directory
  - Purpose
  - Key Contents
* - `nemo_automodel/`
  - **Core Package**
  - Main source code, components, recipes, CLI
* - `examples/`
  - **Working Examples**
  - Production-ready YAML configs, sample scripts
* - `tests/`
  - **Quality Assurance**
  - Unit tests, functional tests, CI/CD validation
* - `docs/`
  - **Documentation**
  - User guides, API docs, tutorials, extensions
* - `docker/`
  - **Containerization**
  - Docker images for development and deployment
* - `scripts/`
  - **Development Tools**
  - Utilities for RAG, indexing, preview generation
```

### Key Files

- **`pyproject.toml`** - Package dependencies, build configuration, tool settings
- **`CONTRIBUTING.md`** - Guidelines for contributors, development setup
- **`LICENSE`** - Apache 2.0 open source license
- **`CHANGELOG.md`** - Version history and release notes

### Examples Directory

The `examples/` directory contains production-ready YAML configurations and sample scripts that demonstrate how to use NeMo Automodel for various training scenarios.

#### Structure

```
examples/
â”œâ”€â”€ llm/
â”‚   â”œâ”€â”€ finetune.py                     # LLM fine-tuning script
â”‚   â”œâ”€â”€ llama_3_2_1b_hellaswag_fp8.yaml # Llama with FP8 quantization
â”‚   â”œâ”€â”€ llama_3_2_1b_hellaswag_nvfsdp.yaml # Llama with nvFSDP
â”‚   â””â”€â”€ *.yaml                          # Additional model configurations
â””â”€â”€ vlm/
    â”œâ”€â”€ finetune.py                     # VLM fine-tuning script
    â”œâ”€â”€ gemma_3_vl_4b_cord_v2_nvfsdp.yaml # Gemma VL with nvFSDP
    â”œâ”€â”€ gemma_3_vl_4b_cord_v2_peft.yaml   # Gemma VL with PEFT
    â””â”€â”€ *.yaml                          # Additional VLM configurations
```

#### Usage

Each YAML configuration can be run directly with the provided scripts:

```bash
# LLM fine-tuning example
python examples/llm/finetune.py --config examples/llm/llama_3_2_1b_hellaswag_nvfsdp.yaml

# VLM fine-tuning example  
python examples/vlm/finetune.py --config examples/vlm/gemma_3_vl_4b_cord_v2_peft.yaml
```

### Testing Structure

The `tests/` directory contains comprehensive test suites that validate all aspects of NeMo Automodel, from individual components to full end-to-end workflows.

#### Test Organization

```
tests/
â”œâ”€â”€ unit_tests/                    # Component-level testing
â”‚   â”œâ”€â”€ _cli/                      # CLI module tests
â”‚   â”œâ”€â”€ _peft/                     # PEFT implementation tests
â”‚   â”œâ”€â”€ _transformers/             # Model wrapper tests
â”‚   â”œâ”€â”€ checkpoint/                # Checkpointing logic tests
â”‚   â”œâ”€â”€ config/                    # Configuration system tests
â”‚   â”œâ”€â”€ datasets/                  # Dataset loader tests
â”‚   â”œâ”€â”€ distributed/               # Distributed training tests
â”‚   â”œâ”€â”€ launcher/                  # Job launcher tests
â”‚   â”œâ”€â”€ loggers/                   # Logging utility tests
â”‚   â”œâ”€â”€ loss/                      # Loss function tests
â”‚   â”œâ”€â”€ optim/                     # Optimizer tests
â”‚   â”œâ”€â”€ quantization/              # FP8 quantization tests
â”‚   â”œâ”€â”€ recipes/                   # Recipe logic tests
â”‚   â”œâ”€â”€ shared/                    # Shared utility tests
â”‚   â”œâ”€â”€ training/                  # Training utility tests
â”‚   â””â”€â”€ utils/                     # General utility tests
â””â”€â”€ functional_tests/              # End-to-end integration testing
    â”œâ”€â”€ checkpoint/                # Checkpoint format validation
    â”œâ”€â”€ hf_consolidated_fsdp/      # HuggingFace + FSDP integration
    â”œâ”€â”€ hf_dcp/                    # Distributed checkpoint testing
    â”œâ”€â”€ hf_peft/                   # PEFT integration testing
    â”œâ”€â”€ hf_transformer/            # Transformer integration tests
    â”œâ”€â”€ hf_transformer_finetune/   # Fine-tuning workflow tests
    â”œâ”€â”€ hf_transformer_llm/        # LLM-specific tests
    â””â”€â”€ hf_transformer_vlm/        # VLM-specific tests
```

#### Running Tests

```bash
# Run all unit tests
python -m pytest tests/unit_tests/

# Run functional tests (requires GPU)
python -m pytest tests/functional_tests/

# Run specific component tests
python -m pytest tests/unit_tests/datasets/
```

## Package Structure

NeMo Automodel is organized as a modular Python package with clear separation of concerns and well-defined interfaces between components. The package follows NVIDIA's established patterns for ML frameworks while optimizing for the specific needs of fine-tuning and training workflows.

### Complete Package Directory Structure

```text
nemo_automodel/
â”œâ”€â”€ __init__.py                    # Package entry point with lazy loading
â”œâ”€â”€ package_info.py               # Version and metadata
â”œâ”€â”€ _cli/                         # Command-line interface
â”‚   â””â”€â”€ app.py                    # Main CLI application
â”œâ”€â”€ components/                   # Core modular components
â”‚   â”œâ”€â”€ _transformers/            # Hugging Face model integration
â”‚   â”‚   â”œâ”€â”€ auto_model.py         # NeMoAutoModelForCausalLM, NeMoAutoModelForImageTextToText
â”‚   â”‚   â””â”€â”€ utils.py              # Transformer utilities
â”‚   â”œâ”€â”€ _peft/                    # Parameter-efficient fine-tuning
â”‚   â”‚   â”œâ”€â”€ lora.py               # LoRA implementation
â”‚   â”‚   â”œâ”€â”€ lora_kernel.py        # Optimized LoRA kernels
â”‚   â”‚   â””â”€â”€ module_matcher.py     # Module matching utilities
â”‚   â”œâ”€â”€ datasets/                 # Data loading and processing
â”‚   â”‚   â”œâ”€â”€ utils.py              # Common dataset utilities
â”‚   â”‚   â”œâ”€â”€ llm/                  # Language model datasets
â”‚   â”‚   â”‚   â”œâ”€â”€ column_mapped_text_instruction_dataset.py
â”‚   â”‚   â”‚   â”œâ”€â”€ hellaswag.py      # HellaSwag evaluation dataset
â”‚   â”‚   â”‚   â”œâ”€â”€ squad.py          # SQuAD question answering
â”‚   â”‚   â”‚   â”œâ”€â”€ packed_sequence.py # Packed sequence optimization
â”‚   â”‚   â”‚   â”œâ”€â”€ mock.py           # Mock datasets for testing
â”‚   â”‚   â”‚   â””â”€â”€ mock_packed.py    # Mock packed datasets
â”‚   â”‚   â””â”€â”€ vlm/                  # Vision-language datasets
â”‚   â”‚       â”œâ”€â”€ datasets.py       # VLM dataset implementations
â”‚   â”‚       â”œâ”€â”€ collate_fns.py    # Specialized collation functions
â”‚   â”‚       â””â”€â”€ utils.py          # VLM-specific utilities
â”‚   â”œâ”€â”€ distributed/              # Multi-GPU and distributed training
â”‚   â”‚   â”œâ”€â”€ ddp.py                # Distributed Data Parallel
â”‚   â”‚   â”œâ”€â”€ fsdp2.py              # Fully Sharded Data Parallel v2
â”‚   â”‚   â”œâ”€â”€ nvfsdp.py             # NVIDIA's optimized FSDP
â”‚   â”‚   â”œâ”€â”€ optimized_tp_plans.py # Tensor parallelism strategies
â”‚   â”‚   â”œâ”€â”€ parallelizer.py       # Parallelization orchestration
â”‚   â”‚   â”œâ”€â”€ tensor_utils.py       # Tensor manipulation utilities
â”‚   â”‚   â”œâ”€â”€ grad_utils.py         # Gradient handling
â”‚   â”‚   â”œâ”€â”€ cp_utils.py           # Context parallelism utilities
â”‚   â”‚   â””â”€â”€ init_utils.py         # Distributed initialization
â”‚   â”œâ”€â”€ checkpoint/               # Advanced checkpointing
â”‚   â”‚   â”œâ”€â”€ checkpointing.py      # Main checkpointing logic
â”‚   â”‚   â”œâ”€â”€ stateful_wrappers.py  # Stateful component wrappers
â”‚   â”‚   â”œâ”€â”€ _torch_backports.py   # PyTorch compatibility
â”‚   â”‚   â””â”€â”€ _backports/           # HuggingFace integration backports
â”‚   â”‚       â”œâ”€â”€ filesystem.py     # Filesystem abstractions
â”‚   â”‚       â”œâ”€â”€ hf_storage.py     # HuggingFace storage integration
â”‚   â”‚       â”œâ”€â”€ hf_utils.py       # HuggingFace utilities
â”‚   â”‚       â”œâ”€â”€ default_planner.py # Default checkpoint planning
â”‚   â”‚       â”œâ”€â”€ planner_helpers.py # Checkpoint planning utilities
â”‚   â”‚       â”œâ”€â”€ consolidate_hf_safetensors.py # SafeTensors consolidation
â”‚   â”‚       â”œâ”€â”€ _fsspec_filesystem.py # FSSpec filesystem support
â”‚   â”‚       â””â”€â”€ _version.py       # Version compatibility
â”‚   â”œâ”€â”€ loss/                     # Optimized loss functions
â”‚   â”‚   â”œâ”€â”€ chunked_ce.py         # Chunked cross-entropy
â”‚   â”‚   â”œâ”€â”€ linear_ce.py          # Linear cross-entropy
â”‚   â”‚   â”œâ”€â”€ masked_ce.py          # Masked cross-entropy
â”‚   â”‚   â”œâ”€â”€ te_parallel_ce.py     # Transformer Engine parallel CE
â”‚   â”‚   â””â”€â”€ triton/               # Custom Triton kernels
â”‚   â”‚       â””â”€â”€ te_cross_entropy.py # TE cross-entropy kernel
â”‚   â”œâ”€â”€ training/                 # Training utilities and management
â”‚   â”‚   â”œâ”€â”€ rng.py                # Random number generation
â”‚   â”‚   â”œâ”€â”€ step_scheduler.py     # Training step scheduling
â”‚   â”‚   â”œâ”€â”€ timers.py             # Performance timing
â”‚   â”‚   â””â”€â”€ utils.py              # Training utilities
â”‚   â”œâ”€â”€ config/                   # Configuration management
â”‚   â”‚   â”œâ”€â”€ loader.py             # YAML configuration loading
â”‚   â”‚   â””â”€â”€ _arg_parser.py        # Command-line argument parsing
â”‚   â”œâ”€â”€ launcher/                 # Job launcher and cluster integration
â”‚   â”‚   â””â”€â”€ slurm/                # SLURM cluster support
â”‚   â”‚       â”œâ”€â”€ config.py         # SLURM configuration
â”‚   â”‚       â”œâ”€â”€ template.py       # Job template generation
â”‚   â”‚       â””â”€â”€ utils.py          # SLURM utilities
â”‚   â”œâ”€â”€ loggers/                  # Logging and monitoring
â”‚   â”‚   â”œâ”€â”€ wandb_utils.py        # Weights & Biases integration
â”‚   â”‚   â””â”€â”€ log_utils.py          # General logging utilities
â”‚   â”œâ”€â”€ optim/                    # Optimization algorithms
â”‚   â”‚   â””â”€â”€ scheduler.py          # Learning rate schedulers
â”‚   â”œâ”€â”€ quantization/             # Model quantization
â”‚   â”‚   â””â”€â”€ fp8.py                # FP8 quantization support
â”‚   â””â”€â”€ utils/                    # Component utilities
â”‚       â”œâ”€â”€ dist_utils.py         # Distributed utilities
â”‚       â”œâ”€â”€ model_utils.py        # Model manipulation
â”‚       â”œâ”€â”€ sig_utils.py          # Signature utilities
â”‚       â””â”€â”€ yaml_utils.py         # YAML processing
â”œâ”€â”€ recipes/                      # End-to-end training workflows
â”‚   â”œâ”€â”€ base_recipe.py            # Base recipe class
â”‚   â”œâ”€â”€ llm/                      # Language model recipes
â”‚   â”‚   â””â”€â”€ finetune.py           # LLM fine-tuning recipe
â”‚   â””â”€â”€ vlm/                      # Vision-language model recipes
â”‚       â””â”€â”€ finetune.py           # VLM fine-tuning recipe
â””â”€â”€ shared/                       # Cross-component shared utilities
    â”œâ”€â”€ import_utils.py           # Safe import utilities
    â””â”€â”€ utils.py                  # Common utility functions
```

### Module Details

#### Core Components (`components/`)

##### **Model Integration (`_transformers/`)**
- **Purpose**: Bridge between Hugging Face models and NeMo training infrastructure
- **Key Classes**: `NeMoAutoModelForCausalLM`, `NeMoAutoModelForImageTextToText`
- **Features**: Drop-in replacements with optimized kernels and distributed support

##### **Parameter-Efficient Fine-Tuning (`_peft/`)**
- **Purpose**: LoRA and other PEFT implementations with optimized kernels
- **Key Components**: LoRA layers, kernel optimizations, module matching
- **Integration**: Works seamlessly with any supported model architecture

##### **Data Pipeline (`datasets/`)**
- **LLM Datasets**: Instruction datasets, evaluation benchmarks, packed sequences
- **VLM Datasets**: Vision-language datasets with specialized preprocessing
- **Features**: Optimized collation, memory-efficient loading, flexible transforms

##### **Distributed Training (`distributed/`)**
- **Strategies**: DDP, FSDP2, nvFSDP, tensor parallelism
- **Features**: Automatic strategy selection, gradient optimization, communication efficiency
- **Scaling**: Single GPU to multi-node clusters

#### Training Recipes (`recipes/`)

##### **Base Recipe Architecture**
```python
class BaseRecipe:
    def __init__(self, model, dataset, strategy, config):
        # Common initialization
    
    def setup(self):
        # Environment and component setup
    
    def train(self):
        # Training loop implementation
    
    def evaluate(self):
        # Evaluation logic
    
    def checkpoint(self):
        # State management
```

##### **LLM Recipes (`llm/`)**
- **Fine-tuning**: Full and parameter-efficient fine-tuning
- **Optimization**: Automatic mixed precision, gradient accumulation
- **Features**: Model-specific optimizations, memory management

##### **VLM Recipes (`vlm/`)**
- **Vision Language Training**: Multi-modal fine-tuning workflows
- **Data Handling**: Image-text pair processing, efficient batching
- **Memory Optimization**: Large model support with gradient checkpointing

#### Shared Infrastructure

##### **Import Management (`shared/import_utils.py`)**
- **Safe Imports**: Graceful handling of optional dependencies
- **Feature Detection**: Runtime capability discovery
- **Fallbacks**: Alternative implementations when dependencies unavailable

##### **Configuration System (`components/config/`)**
- **YAML-driven**: Human-readable configuration files
- **Validation**: Schema validation and error reporting
- **Templating**: Reusable configuration patterns

##### **CLI Interface (`_cli/`)**
- **Job Launching**: Simple command-line interface for training
- **Environment Detection**: Automatic cluster and GPU detection
- **Configuration**: CLI argument to YAML configuration mapping

## Getting Started Paths

Choose your exploration path based on your role and goals:

::::{grid} 1 1 2 2
:gutter: 2

:::{grid-item-card} {octicon}`mortar-board;1.5em;sd-mr-1` New Users
:class-header: sd-bg-success sd-text-white

**Recommended Path:**
1. ğŸ“ Start with `examples/` - Review working YAML configurations
2. ğŸ§ª Run a simple training example
3. ğŸ“– Read {doc}`/get-started/quick-start`
4. ğŸ”§ Modify configurations for your use case

+++
{bdg-success}`Beginner Friendly`
:::

:::{grid-item-card} {octicon}`tools;1.5em;sd-mr-1` Contributors & Developers
:class-header: sd-bg-primary sd-text-white

**Recommended Path:**
1. ğŸ§© Explore `nemo_automodel/components/` - Understand building blocks
2. ğŸ³ Study `nemo_automodel/recipes/` - See component orchestration
3. ğŸ§ª Run `tests/` - Understand expected behavior
4. ğŸ“‹ Read `CONTRIBUTING.md` - Development guidelines

+++
{bdg-primary}`Technical`
:::

:::{grid-item-card} {octicon}`rocket;1.5em;sd-mr-1` Advanced Users
:class-header: sd-bg-info sd-text-white

**Recommended Path:**
1. ğŸ“¦ Study package architecture - Component design patterns
2. ğŸ”§ Examine distributed training strategies
3. âš¡ Understand optimization techniques
4. ğŸ—ï¸ Create custom components or recipes

+++
{bdg-info}`Advanced`
:::

:::{grid-item-card} {octicon}`server;1.5em;sd-mr-1` Infrastructure Teams
:class-header: sd-bg-warning sd-text-white

**Recommended Path:**
1. ğŸ³ Review `docker/` - Container setup
2. ğŸ¯ Understand launcher and SLURM integration
3. âš–ï¸ Study distributed training strategies
4. ğŸ“Š Examine logging and monitoring setup

+++
{bdg-warning}`Operations`
:::

::::

### Core Package Structure
The Automodel source code is available under the [`nemo_automodel`](https://github.com/NVIDIA-NeMo/Automodel/tree/main/nemo_automodel) directory. It is organized into three main directories:
- [`components/`](https://github.com/NVIDIA-NeMo/Automodel/tree/main/nemo_automodel/components) - Self-contained modules
- [`recipes/`](https://github.com/NVIDIA-NeMo/Automodel/tree/main/nemo_automodel/recipes) - End-to-end training workflows
- [`_cli/`](https://github.com/NVIDIA-NeMo/Automodel/tree/main/nemo_automodel/_cli) - Command-line interface

#### Components Directory
The `components/` directory contains isolated modules used in training loops. Each component is designed to be dependency-light and reusable without cross-module imports.

```
$ tree -L 1 nemo_automodel/components/

â”œâ”€â”€ _peft/          - Implementations of PEFT methods, such as LoRA.
â”œâ”€â”€ _transformers/  - Optimized model implementations for Hugging Face models.
â”œâ”€â”€ checkpoint/     - Checkpoint save and load-related logic.
â”œâ”€â”€ config/         - Utils to load YAML files and CLI-parsing helpers.
â”œâ”€â”€ datasets/       - LLM and VLM datasets and utils (collate functions, preprocessing).
â”œâ”€â”€ distributed/    - Distributed processing primitives (DDP, FSDP2, nvFSDP).
â”œâ”€â”€ launcher/       - Job launcher for interactive and batch (Slurm, K8s) processing.
â”œâ”€â”€ loggers/        - Metric/event logging for Weights & Biases and other tools
â”œâ”€â”€ loss/           - Loss functions (such as cross-entropy and linear cross-entropy, etc.).
â”œâ”€â”€ optim/          - Optimizers and LR schedulers, including fused or second-order variants.
â”œâ”€â”€ training/       - Training and fine-tuning utils.
â””â”€â”€ utils/          - Small, dependency-free helpers (seed, profiler, timing, fs).
```

#### Key Component Features
- Each component can be used independently in other projects
- Each component has its own dependencies, without cross-module imports
- Unit tests are colocated with the component they cover

#### Recipes Directory
Recipes define **end-to-end workflows** (data â†’ training â†’ eval) for a variety of tasks, combining components into usable pipelines.

```
$ tree -L 2 nemo_automodel/recipes/
â”œâ”€â”€ llm
â”‚   â””â”€â”€ finetune.py   - Finetune recipe for LLMs (SFT, PEFT).
â””â”€â”€ vlm
    â””â”€â”€ finetune.py   - Finetune recipe for VLMs (SFT, PEFT).
```

For configuration examples and running instructions, see {ref}`get-started-quick-start` and the LLM SFT guide.

#### CLI Directory
The `automodel` CLI simplifies job execution across environments. See the Quick Start guide for basic examples and the SLURM launcher guide for cluster usage.

## Development Patterns

### Component Design Principles

#### **Independence**
- Each component has minimal external dependencies
- Clear interfaces with well-defined contracts
- Self-contained testing and validation

#### **Composability**
- Components work together through standard interfaces
- Mix-and-match different implementations
- Recipe-level orchestration of component interactions

#### **Extensibility**
- New components follow established patterns
- Inheritance from base classes
- Plugin-style architecture for custom implementations

### Import Architecture

#### **Lazy Loading**
```python
def __getattr__(name: str):
    if name in __all__:
        module = importlib.import_module(f"{__name__}.{name}")
        globals()[name] = module
        return module
    raise AttributeError(f"module {__name__!r} has no attribute {name!r}")
```

#### **Top-level Promotion**
Key classes are promoted to package level for convenient access via the main package namespace.

#### **Safe Dependencies**
Optional dependencies handled gracefully:
```python
try:
    from optional_dependency import SomeClass
    HAS_OPTIONAL = True
except ImportError:
    HAS_OPTIONAL = False
    SomeClass = None
```

### Configuration Patterns

#### **YAML-First Design**
- All training parameters in YAML files
- Runtime override capabilities
- Environment variable substitution
- Validation and error reporting

#### **Hierarchical Structure**
```yaml
model:
  name: "meta-llama/Llama-3.2-1B"
  config_overrides:
    attention_dropout: 0.1

data:
  dataset_type: "instruction"
  batch_size: 32

training:
  optimizer: "adamw"
  learning_rate: 3e-4
  num_epochs: 3

distributed:
  strategy: "fsdp2"
  tensor_parallel_size: 1
```

### Best Practices for Contributors

#### Adding New Components

1. **Follow Naming Conventions**: Use descriptive, consistent names
2. **Implement Base Interfaces**: Inherit from established base classes
3. **Add Comprehensive Tests**: Unit tests and integration tests
4. **Document APIs**: Clear docstrings with examples
5. **Update Configuration**: Add config options if needed

#### Testing Patterns

- **Unit Tests**: Test individual component functionality
- **Integration Tests**: Test component interactions
- **End-to-End Tests**: Test complete training workflows
- **Performance Tests**: Validate scaling and memory usage

#### Documentation Requirements

- **API Documentation**: Auto-generated from docstrings
- **Usage Examples**: Practical code examples
- **Configuration Reference**: All available options
- **Migration Guides**: For breaking changes

## Development Workflow

Understanding the repository structure helps with effective development:

1. **Start with Examples**: Use `examples/` to understand expected usage patterns
2. **Modify Components**: Make changes in `nemo_automodel/components/` for new features
3. **Update Recipes**: Modify `nemo_automodel/recipes/` for workflow changes
4. **Add Tests**: Create tests in `tests/` that mirror your changes
5. **Update Documentation**: Modify `docs/` to reflect new features
6. **Container Testing**: Use `docker/` for reproducible testing environments

## Summary

NeMo Automodel's structure is designed for developer productivity, component reusability, and production scalability. The modular architecture enables rapid prototyping while the well-defined interfaces ensure reliable integration patterns. This structure supports the framework's goal of providing immediate access to new models with enterprise-grade training capabilities.

### Quick Reference

- **Repository Overview**: Start with `examples/` and `README.md`
- **Package Internals**: Explore `nemo_automodel/components/` and `recipes/`
- **Testing**: Run tests in `tests/` to understand behavior
- **Documentation**: Comprehensive guides in `docs/`
- **Development**: Follow patterns in `CONTRIBUTING.md`
- **Deployment**: Use containers in `docker/` for consistent environments
