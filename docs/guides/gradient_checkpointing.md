# ðŸš€ Gradient (Activation) Checkpointing in NeMo-AutoModel

Gradient checkpointing - also called _activation checkpointing_ - trades a little extra compute for a **large reduction in GPU memory** by recomputing intermediate activations during the backwards pass instead of storing them.  
It is especially powerful when combined with memory-efficient loss functions (e.g. Linear-Cut Cross-Entropy) and parameter sharding via FSDP.

---

## 1. Enabling Gradient Checkpointing

### 1.1. In YAML config
Add the `activation_checkpointing: true` flag under your distributed strategy.  
Example (snippet):

```yaml
# examples/llm/llama_3_2_1b_my_finetune.yaml
...
# NVFSDP
distributed:
  _target_: nemo_automodel.components.distributed.nvfsdp.NVFSDPManager
  tp_size: 2                 # Tensor Parallel = 2
  dp_size: 4                 # Data Parallel   = 4
  activation_checkpointing: true   # <-- enable GC
  sequence_parallel: false
  ...

# FSDP-2 (upstream PyTorch â‰¥ 2.3)
# Uncomment this block *instead* if you wish to stay on stock PyTorch FSDP-2
# distributed:
#   _target_: nemo_automodel.components.distributed.fsdp2.FSDP2Manager
#   tp_size: 2
#   dp_size: 4
#   activation_checkpointing: true
#   sequence_parallel: false
#   ...
```

If you are using the FSDP2 strategy (for PyTorch 2.3+), set the flag in exactly the same place - the underlying manager will forward it to the `fsdp2_strategy_parallelize(...)` helper.

### 1.2. Programmatically
```python
from nemo_automodel.components.distributed.nvfsdp import NVFSDPManager
# or for FSDP2
# from nemo_automodel.components.distributed.fsdp2 import FSDP2Manager

# -- NVFSDP --
nv_manager = NVFSDPManager(tp_size=2, dp_size=4, activation_checkpointing=True)
model, optimizer = nv_manager.parallelize(model, optimizer)

# -- OR: FSDP-2 --
# fsdp2_manager = FSDP2Manager(tp_size=2, dp_size=4, activation_checkpointing=True)
# model = fsdp2_manager.parallelize(model)
```

---

## 2. Combining with Linear-Cut Cross-Entropy (LC-CE)

Linear-Cut Cross-Entropy (LC-CE) reduces the hidden-state memory required to compute the loss by calculating the softmax on-the-fly, thus avoiding to allocate memory for the logits.
It is already available via `nemo_automodel.components.loss.linear_ce.FusedLinearCrossEntropy` and can be enabled in recipes by using the following:

```yaml
loss_fn:
  _target_: nemo_automodel.components.loss.linear_ce.FusedLinearCrossEntropy
```

LC-CE and gradient checkpointing target **different memory hot-spots** (output layer vs transformer blocks) so their benefits stack almost linearly.

---

## 3. Example Memory Savings (A100-80GB, Llama-3-8B)
| Technique | Max GPU Mem (GB) | Î” vs Baseline |
|-----------|-----------------|---------------|
| Baseline (no sharding) | 71.2 | - |
| + FSDP (dp=4, tp=2) | 22.5 | â†“ 68 % |
| + LC-CE | 19.3 | â†“ 73 % |
| + Gradient Checkpointing | 12.1 | â†“ 83 % |
| **FSDP + LC-CE + Checkpointing** | **7.9** | **â†“ 89 %** |

Notes:
* Measurements taken with batch size = 4, sequence len = 2048, AdamW, PyTorch 2.3.
* Peak memory reported by `torch.cuda.max_memory_allocated()` averaged across DP ranks.
* Expect Â±5 % variance depending on exact model, sequence length and GPU architecture.

---

## 4. Performance Considerations
1. **Extra compute** - Each checkpointed segment is recomputed once during the backward pass. In practice the wall-clock overhead is â‰ˆ 5-10 % for transformer models.
2. **Throughput vs Batch Size** - The goal is usually to _increase batch size_ or _sequence length_ while keeping throughput constant.
3. **Selective Checkpointing** - For very long models you can checkpoint every _k_-th layer by replacing the boolean flag with an integer (e.g. `activation_checkpointing: 2` â†’ every second layer). This is exposed via the same flag in the YAML.

---

## 5. Verifying It Works
Run your training script with `CUDA_VISIBLE_DEVICES=0` and inspect the peak memory:
```bash
CUDA_VISIBLE_DEVICES=0 python -m torch.distributed.run \
    --nproc-per-node 1 \
    nemo_automodel/recipes/llm/finetune.py \
    -c examples/llm/llama_3_2_1b_my_finetune.yaml
```
Look for a log line similar to:
```
[rank0] peak memory: 7.9 GB (activation ckpt = on, lc-ce = on, fsdp = on)
```

---

Happy fine-tuning! ðŸŒŸ 